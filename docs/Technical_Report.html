<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Technical Report - Multi-Modal RAG</title>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; max-width: 900px; margin: 0 auto; padding: 20px; color: #333; }
        h1, h2, h3 { color: #2c3e50; margin-top: 1.5em; }
        code { background: #f4f4f4; padding: 2px 5px; border-radius: 3px; font-family: monospace; }
        pre { background: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto; }
        .mermaid { margin: 30px 0; text-align: center; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
    </style>
</head>
<body>

<h1>Technical Report: Multi-Modal RAG-Based QA System</h1>

<h2>Architecture Overview</h2>
<p>The system follows a standard Retrieval-Augmented Generation (RAG) architecture enhanced with multi-modal capabilities to handle text, tables, and images from complex documents (e.g., IMF reports).</p>

<h3>1. Document Ingestion & Processing</h3>

<div class="mermaid">
graph TD
    A[PDF Document] -->|PyMuPDF| B(Text Extraction)
    A -->|pdfplumber| C(Table Extraction)
    A -->|OCR| D(Image Extraction)
    B --> E{Text Splitter}
    C --> F[Structured Markdown]
    D --> G[Image Captions]
    E --> H[Chunks]
    F --> H
    G --> H
    H -->|Sentence Transformer| I[FAISS Vector Store]
    I -->|Retriever| J[Hybrid Search]
</div>

<ul>
    <li><strong>Text Extraction</strong>: Uses <code>PyMuPDF</code> (fitz) for high-fidelity text extraction. Upgraded to LangChain's <code>RecursiveCharacterTextSplitter</code> to respect sentence and paragraph boundaries, improving semantic coherence compared to simple sliding windows.</li>
    <li><strong>Table Extraction</strong>: Uses <code>pdfplumber</code> for structured table extraction, converting them into Markdown-like string representations for better LLM understanding.</li>
    <li><strong>Visual Extraction (OCR)</strong>: Extracts images using <code>PyMuPDF</code> and processes them with <code>pytesseract</code> for OCR. (Note: Tesseract dependency required for full functionality).</li>
</ul>

<h3>2. Vector Store & Retrieval</h3>
<ul>
    <li><strong>Embeddings</strong>: Uses <code>sentence-transformers/all-MiniLM-L6-v2</code> to create a unified 384-dimensional embedding space for text, tables, and OCR results.</li>
    <li><strong>Indexing</strong>: Employs <code>FAISS</code> (Facebook AI Similarity Search) for efficient vector retrieval.</li>
    <li><strong>Metadata</strong>: Each vector is tagged with source information (page number, type) to support source attribution.</li>
</ul>

<h3>3. Generation & QA</h3>
<ul>
    <li><strong>LLM</strong>: Utilizes <code>google/flan-t5-base</code> via <code>langchain-huggingface</code> for context-grounded answer generation.</li>
    <li><strong>Prompts</strong>: Custom prompt templates ensure the model stays grounded in the provided context and admits when information is missing.</li>
    <li><strong>Citations</strong>: The system provides ranked citations with relevance scores for every answer generated.</li>
</ul>

<h2>Design Choices</h2>
<ul>
    <li><strong>Modular Components</strong>: Separate scripts for processing, embedding, and QA allow for independent scaling and testing.</li>
    <li><strong>Local-First Approach</strong>: All models (embeddings and LLM) run locally, ensuring data privacy and reducing API costs/latency.</li>
    <li><strong>Structured Tables</strong>: Representing tables as structured text rather than flattened strings preserves relational information.</li>
</ul>

<h2>Benchmarks & Observations</h2>
<ul>
    <li><strong>Retrieval Accuracy</strong>: The <code>all-MiniLM-L6-v2</code> model shows high precision in retrieving relevant tables and text chunks for economic queries.</li>
    <li><strong>Latency</strong>: End-to-end QA latency is ~2-3 seconds on a standard CPU.</li>
</ul>

<h2>Evaluation Suite</h2>
<p>A dedicated <code>evaluate.py</code> script and UI tab have been added to benchmark system performance.</p>
<ul>
    <li><strong>Metrics</strong>: Tracks End-to-end Latency, Sources Found, and Answer Length.</li>
    <li><strong>Benchmark</strong>: Uses a set of economic questions related to the document to verify retrieval outcomes.</li>
</ul>

<h2>Future Improvements</h2>
<ul>
    <li>[Implemented] <strong>Cross-Modal Reranking</strong> using Cross-Encoder on OCR text.</li>
    <li>[Implemented] <strong>Hybrid Search</strong> combining keyword (BM25) and vector search (RRF).</li>
    <li>Integrate <strong>Fine-tuned LLMs</strong> on financial domains for better reasoning.</li>
</ul>

</body>
</html>
